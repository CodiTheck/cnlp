from .base import BaseAlgorithm as BaseAlgo
from .utils import imp
from . import LOG
from . import GSET


class TXMAlgorithm(BaseAlgo):
    """ 
    Structure of a Texumer algorithm which represents a text summary 
    operation on NLP.
    """
    def __init__(self, **config):
        """ 
        Constructor of an instance of Texumer algorithm
        """
        self._args = config;
        self._res  = None;

    @property
    def results(self):
        """
        Return the results generated by this algo
        """
        return self._res;


class RlvSent(TXMAlgorithm):
    """ 
    Algorithm of relevant sentences extraction from a text 
    """
    def __init__(self, **config):
        super(RlvSent, self).__init__(**config);
        self._scount = config.get('N', -1);

        # Importation of the dependences
        self.__np    = imp('numpy');
        self.__pd    = imp('pandas');
        self.__TFIDF = imp('TfidfVectorizer', frm='sklearn.feature_extraction.text');
        self.__spacy = imp("spacy");

    def _f_(self, *args, **kwargs):
        """ 
        Main function of the relevant sentences algorithm.
        """
        np    = self.__np;
        pd    = self.__pd;
        TFIDF = self.__TFIDF;
        spacy = self.__spacy;
        argc = len(args);

        if argc >= 1:
            # This argument is a text.
            text = args[0];

            # we check the language processed
            LOG('Loading and configuration pre-trainned model of the language processed...');
            nlp  = None;
            lang = None;
            if GSET['LANGUAGE_PROCESSED'] == 'FR':
                # if the processed language is french, then we load 'fr_core_news_md'
                nlp  = spacy.load('fr_core_news_md');
                lang = 'french';
            elif GSET['LANGUAGE_PROCESSED'] == 'EN':
                # if the processed language is french, then we load 'fr_core_news_md'
                nlp  = spacy.load('en_core_web_md');
                lang = 'english';

            # nlp.add_pipe(nlp.create_pipe('sentencizer'));

            # spacy doc building
            LOG("Matching of the text to sentences ...");
            doc = nlp(text.replace('\n', ' '));
            sents = [sent.text.strip() for sent in doc.sents];

            #  tokenize of the sentences
            sents_dict = {sent:num for num, sent in enumerate(sents)};
            print("{} sentences found.".format(len(sents_dict)));

            # calculation of TF-IDF
            LOG("Calculation of TF-IDF for each word contained into sentences...");
            sent_vectors = self._calculate_tfidf(lang, sents);
            print("sent_vectors =>", sent_vectors);

            # Getting sentence scores for each sentences
            LOG("Calculation of score for each sentences...");
            sent_scores = np.array(sent_vectors.sum(axis=1));
            sent_scores = sent_scores.ravel();
            print(sent_scores);

            # Getting sentences list for the summary
            LOG("Getting of {} last sentence(s) for the summary...".format(self._scount));
            self._res = self._select_n_sents(sents, sent_scores);
        return self._res;

    def _calculate_tfidf(self, lang, sents):
        """ 
        Function of TF-IDF calculation for each word contained into sentences. 
        """
        TFIDF = self.__TFIDF;
        tfidf_params = {
                'min_df':           2,
                'max_features':     None,
                'strip_accents':    'unicode',
                'analyzer':         'word',
                'token_pattern':    '\w{1,}',
                'ngram_range':      (1, 3),
                'use_idf':          1,
                'smooth_idf':       1,
                'sublinear_tf':     1,
        };
        if lang == 'english':
            tfidf_params['stop_words'] = lang;

        tf_idf_vec   = TFIDF(**tfidf_params);
        tf_idf_vec.fit(sents);
        return tf_idf_vec.transform(sents);

    def _select_n_sents(self, s, sc):
        """ 
        Function of selection of N-last sentences according their scores.
        """
        np = self.__np;
        n  = self._scount if self._scount > 0 else round(len(s) / 4);
        sc = np.argsort(sc, axis=0);
        inds = [i for i in sc[::-1][:n]];
        res  = [];
        for i, sent in enumerate(s):
            if i in inds:
                res.append(sent);
        return res;
        

    
