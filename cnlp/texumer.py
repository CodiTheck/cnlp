from .base import BaseAlgorithm as BaseAlgo
from .utils import imp
from . import LOG
from . import GSET


class TXMAlgorithm(BaseAlgo):
    """Text summary algorithm

    Structure of a Texumer algorithm which represents a text summary 
    operation on NLP.
    """

    def __init__(self, **config):
        """Constructor of an instance of Texumer algorithm.
        Args:
            **config: config variables.
        """
        self._args = config
        self._res = None

    @property
    def results(self):
        """str: Return the results generated by this algo."""
        return self._res


class RlvSent(TXMAlgorithm):
    """Algorithm of relevant sentences extraction from a text 
    """

    def __init__(self, **config):
        """Constructor of an instance of RlvSent algorithm.
        Args:
            **config: config variables.

        Raises:
            AttributeError: If the required modules has not required 
                attributs.
            ModuleNotFoundError: If the required modules is not found.
        """
        super(RlvSent, self).__init__(**config);
        self._scount = config.get('N', -1)

        # Importation of the dependences
        self.__np = imp('numpy')
        self.__pd = imp('pandas')
        self.__TFIDF = imp('TfidfVectorizer',
                           frm='sklearn.feature_extraction.text')
        self.__spacy = imp("spacy")

    def _f_(self, *args, **kwargs):
        """Main function of the relevant sentences algorithm.

        Args:
            *args: Variable length argument list.
            **kwargs: Additional keyword arguments.

        Returns:
            mixed: Return a result after processing.
        """
        np = self.__np
        pd = self.__pd
        TFIDF = self.__TFIDF
        spacy = self.__spacy
        argc = len(args)

        if argc >= 1:
            # This argument is a text.
            text = args[0]

            # we check the language processed
            LOG(
                'Loading and configuration pre-trainned'
                ' model of the language processed...'
            )
            nlp  = None
            lang = None
            if GSET['LANGUAGE_PROCESSED'] == 'FR':
                # if the processed language is french, 
                # then we load 'fr_core_news_md'
                nlp = spacy.load('fr_core_news_md')
                lang = 'french'
            elif GSET['LANGUAGE_PROCESSED'] == 'EN':
                # if the processed language is french,
                # then we load 'fr_core_news_md'
                nlp = spacy.load('en_core_web_md')
                lang = 'english'

            # nlp.add_pipe(nlp.create_pipe('sentencizer'));

            # spacy doc building
            LOG("Matching of the text to sentences ...")
            doc = nlp(text.replace('\n', ' '))
            sents = [sent.text.strip() for sent in doc.sents]

            #  tokenize of the sentences
            sents_dict = {sent:num for num, sent in enumerate(sents)}
            print("{} sentences found.".format(len(sents_dict)))

            # calculation of TF-IDF
            LOG(
                "Calculation of TF-IDF for each word contained"
                " into sentences..."
            )
            sent_vectors = self._calculate_tfidf(lang, sents)
            print("sent_vectors =>", sent_vectors)

            # Getting sentence scores for each sentences
            LOG("Calculation of score for each sentences...")
            sent_scores = np.array(sent_vectors.sum(axis=1))
            sent_scores = sent_scores.ravel()
            print(sent_scores)

            # Getting sentences list for the summary
            LOG("Getting of {} last sentence(s) for the summary..."\
                .format(self._scount))
            self._res = self._select_n_sents(sents, sent_scores)
        return self._res

    def _calculate_tfidf(self, lang, sents):
        """TF-IDF calculation.

        Function of TF-IDF calculation for each word contained 
        into sentences.

        Args:
            lang (str): String of language.
            sents (list of :obj:`str`): The list of the clean sentenses.

        Returns:
            mixed: The transformation results of TF-IDF calculation.
        """
        TFIDF = self.__TFIDF
        tfidf_params = {
                'min_df': 2,
                'max_features': None,
                'strip_accents': 'unicode',
                'analyzer': 'word',
                'token_pattern': '\w{1,}',
                'ngram_range': (1, 3),
                'use_idf': 1,
                'smooth_idf': 1,
                'sublinear_tf': 1,
                }
        if lang == 'english':
            tfidf_params['stop_words'] = lang

        tf_idf_vec = TFIDF(**tfidf_params)
        tf_idf_vec.fit(sents)
        return tf_idf_vec.transform(sents)

    def _select_n_sents(self, s, sc):
        """Selection of N-last.

        Function of selection of N-last sentences according
        their scores.

        Args:
            lang (str): String of language.
            sents (np.ndarray of :obj:`int`): The list of the sentences
                scores.

        Returns:
            (list of :obj:`str`): Return the list of selected
                sentences with the highest scores.
        """
        np = self.__np
        n  = self._scount if self._scount > 0 else round(len(s) / 4)
        sc = np.argsort(sc, axis=0)
        inds = [i for i in sc[::-1][:n]]
        res  = []
        for i, sent in enumerate(s):
            if i in inds:
                res.append(sent)
        return res
    
